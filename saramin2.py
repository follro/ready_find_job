# -*- coding: utf-8 -*-
"""saramin2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14EUmImQbZcNf3SQ6HeelFX9gNAS0ub30
"""

from IPython.utils.text import dedent
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import NoSuchElementException
from bs4 import BeautifulSoup

import pandas as pd

service = Service(executable_path=r'/usr/bin/chromedriver')
options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
}
options.add_argument(f"user-agent={headers['User-Agent']}")
driver = webdriver.Chrome(service=service, options=options)

# 딕셔너리 key에 value 저장하는 함수
def update_dic(dic, key_value_list):
   dic_keys = list(dic.keys())
   keys_in_not_dic = [key for key in dic_keys if key not in [k for k, v in key_value_list]]
   for key, value in key_value_list:
      dic[key].append(value)
   for key in keys_in_not_dic:
      dic[key].append(" ")

# 페이지 수 설정
total_pages = 10

# 데이터를 저장할 빈 리스트 생성
data = []

for page in range(1, total_pages + 1):
    # 웹 페이지 URL 설정
    url = f"https://www.saramin.co.kr/zf_user/search?cat_mcls=2&exp_cd=1&job_type=1%2C4&company_cd=0%2C1%2C2%2C3%2C4%2C5%2C6%2C7%2C9%2C10&panel_type=&search_optional_item=y&search_done=y&panel_count=y&preview=y&recruitPage={page}&recruitSort=relation&recruitPageCount=40&inner_com_type=&searchword=&show_applied=&quick_apply=&except_read=&ai_head_hunting=&mainSearch=n"

    # 웹 페이지에 접속
    driver.get(url)

    # 페이지 HTML 가져오기
    html = driver.page_source

    # BeautifulSoup을 사용하여 HTML 파싱
    soup = BeautifulSoup(html, "html.parser")

    # id="recruit_info_list"인 요소 찾음
    div_recruit = driver.find_element(By.ID, "recruit_info_list")

    # dic_recruit에서 요소들 찾음
    name = div_recruit.find_elements(By.CLASS_NAME,"corp_name")
    tit = div_recruit.find_elements(By.CLASS_NAME, "job_tit")
    date = div_recruit.find_elements(By.CLASS_NAME, "date")
    href = div_recruit.find_elements(By.XPATH, "//a[@class='data_layer']")

    # 리스트, 딕셔너리 생성
    list_name = [item.text.strip() for item in name]
    list_tit = [item.text.strip() for item in tit]
    list_date = [item.text.strip() for item in date]
    list_href = [item.get_attribute("href") for item in href]
    list_logo = []
    list_key_value_pairs = []
    dic_info = {
       "기업형태": [], "사원수": [],  "업종": [],
       "설립일": [],  "매출액": [], "대표자명": [],
       "홈페이지": [], "기업주소": []
    }
    dic_cont = {
       "경력": [], "학력": [], "근무형태": [],
       "급여": [], "근무지역": [], "직급/직책": [],
       "근무일시": [], "우대사항": [], "필수사항": []
    }

    for link in list_href:
       driver.get(link)
       driver.implicitly_wait(time_to_wait=10)
       xpath_cont = driver.find_element(By.XPATH, "//*[@id='content']/div[3]/section[1]/div[1]")
       div_company = xpath_cont.find_element(By.CLASS_NAME, "company")
       # img 요소 찾음 없으면 공백문자 append
       try:
          div_logo = xpath_cont.find_element(By.CLASS_NAME, "logo")
          img_logo = div_logo.find_element(By.CSS_SELECTOR, "img")
          list_logo.append(img_logo.get_attribute('src'))
       except NoSuchElementException:
          list_logo.append(" ")
       # 기업정보 요소들 찾음 dt 요소를 key, dd 요소를 value 로 리스트에 저장한 뒤 딕셔너리와 비교해서 key 존재하면 해당 key 에 value 대응 없으면 공백문자
       try:
          div_tit = xpath_cont.find_element(By.CLASS_NAME, "title")
          div_info = xpath_cont.find_element(By.XPATH, "//div[@class='info']")
          dl_elements = div_info.find_elements(By.CSS_SELECTOR, "dl")
          for dl_element in dl_elements:
             dt = dl_element.find_element(By.CSS_SELECTOR, "dt")
             dd = dl_element.find_element(By.CSS_SELECTOR, "dd")
             key = dt.text.replace("*","")
             value = dd.text.strip()
             list_key_value_pairs.append((key, value))
          update_dic(dic_info, list_key_value_pairs)
          list_key_value_pairs.clear()
       except NoSuchElementException:
          for key in dic_info:
             dic_info[key].append(" ")
       # 경력 등 요소들 찾음 기업정보 요소들 찾을 때와 같은 방법으로 처리
       try:
          div_cont = xpath_cont.find_element(By.XPATH, "//div[@class='cont']")
          dl_cont_elements = div_cont.find_elements(By.CSS_SELECTOR, "dl")
          for dl_element in dl_cont_elements:
             dt = dl_element.find_element(By.CSS_SELECTOR, "dt")
             dd = dl_element.find_element(By.CSS_SELECTOR, "dd")
             key = dt.text.strip()
             value = dd.text.replace("상세보기","").strip()
             list_key_value_pairs.append((key,value))
          update_dic(dic_cont, list_key_value_pairs)
          list_key_value_pairs.clear()
       except NoSuchElementException:
          for key in dic_cont:
             dic_cont[key].append(" ")

    # data 리스트에 요소들 저장
    for i, (corp_name, corp_tit, corp_date, corp_href, corp_logo) in enumerate(zip(list_name, list_tit, list_date, list_href, list_logo)):
        data.append({
            "회사명": corp_name,
            "채용공고": corp_tit,
            "마감일": corp_date,
            "href": corp_href,
            "logo": corp_logo,
            "기업형태": dic_info["기업형태"][i],
            "사원수": dic_info["사원수"][i],
            "업종": dic_info["업종"][i],
            "설립일": dic_info["설립일"][i],
            "매출액": dic_info["매출액"][i],
            "대표자명": dic_info["대표자명"][i],
            "홈페이지": dic_info["홈페이지"][i],
            "기업주소": dic_info["기업주소"][i],
            "경력": dic_cont["경력"][i],
            "학력": dic_cont["학력"][i],
            "근무형태": dic_cont["근무형태"][i]
        })

# 웹 드라이버 종료
driver.quit()

# 데이터를 CSV 파일로 저장
df = pd.DataFrame(data, columns=["채용공고", "회사명", "마감일", "href", "logo", "기업형태", "사원수", "업종", "설립일", "매출액", "대표자명", "홈페이지", "기업주소", "경력", "학력", "근무형태"])
df.to_csv("it개발, 데이터.csv", index=False, encoding="utf-8-sig")